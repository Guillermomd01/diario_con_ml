{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp313-cp313-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\guillermo\\diario_perosnal_ml\\venv\\lib\\site-packages (from scikit-learn) (2.2.4)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.2-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp313-cp313-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   ------------------------------ --------- 8.4/11.1 MB 41.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 35.0 MB/s eta 0:00:00\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.15.2-cp313-cp313-win_amd64.whl (41.0 MB)\n",
      "   ---------------------------------------- 0.0/41.0 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 9.4/41.0 MB 45.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 19.7/41.0 MB 46.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 29.9/41.0 MB 46.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  40.1/41.0 MB 46.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 41.0/41.0 MB 38.2 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el CSV\n",
    "import pandas as pd\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# Cargar el archivo original\n",
    "df = pd.read_csv(r'C:\\Users\\Guillermo\\Downloads\\archive\\text.csv')\n",
    "\n",
    "\n",
    "#creamos un dataframe con los misma cantidad de ejemplos para cada emocion\n",
    "label0 = df[df['label'] == 0].sample(n=1000, random_state=1)\n",
    "label1 = df[df['label'] == 1].sample(n=1000, random_state=1)\n",
    "label2 = df[df['label'] == 2].sample(n=1000, random_state=1)\n",
    "label3 = df[df['label'] == 3].sample(n=1000, random_state=1)\n",
    "label4 = df[df['label'] == 4].sample(n=1000, random_state=1)\n",
    "label5 = df[df['label'] == 5].sample(n=1000, random_state=1)\n",
    "\n",
    "df_balanced = pd.concat([label0, label1, label2, label3, label4, label5], ignore_index=True)\n",
    "\n",
    "# Traducir toda la columna 'text' al español\n",
    "df_balanced['text'] = df_balanced['text'].head(6000).apply(lambda x: GoogleTranslator(source='auto', target='es').translate(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.725\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.64      0.69       193\n",
      "           1       0.61      0.69      0.65       203\n",
      "           2       0.63      0.75      0.69       198\n",
      "           3       0.78      0.76      0.77       206\n",
      "           4       0.80      0.72      0.76       202\n",
      "           5       0.83      0.79      0.81       198\n",
      "\n",
      "    accuracy                           0.72      1200\n",
      "   macro avg       0.73      0.72      0.73      1200\n",
      "weighted avg       0.73      0.72      0.73      1200\n",
      "\n",
      "[[123  23  21  14   7   5]\n",
      " [  9 140  31  10   9   4]\n",
      " [  7  29 149   6   4   3]\n",
      " [ 13  13  15 156   8   1]\n",
      " [  5  13  10   9 146  19]\n",
      " [  8  10  11   5   8 156]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "\"\"\"\n",
    "sadness (0), joy (1), love (2), anger (3), fear (4), and surprise (5).\n",
    "\"\"\"\n",
    "#df_traducido = df.iloc[:3500,:]\n",
    "\n",
    "X = df_balanced['text']\n",
    "y = df_balanced['label']\n",
    "\n",
    "vector = TfidfVectorizer()\n",
    "X_vectoriced = vector.fit_transform(X)\n",
    "\n",
    "X_train,X_test, y_train, y_test = train_test_split(X_vectoriced,y, test_size=0.2,random_state=34)\n",
    "\n",
    "\n",
    "model_lr = LogisticRegression(max_iter=1000,class_weight='balanced')\n",
    "model_lr.fit(X_train,y_train)\n",
    "print(model_lr.score(X_test,y_test))\n",
    "y_predict = model_lr.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test,y_predict)\n",
    "cr = classification_report(y_test,y_predict)\n",
    "print(cr)\n",
    "print(cm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizador_tfidf.pkl']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "joblib.dump(model_lr, 'modelo_logistico.pkl')\n",
    "\n",
    "# Guardar el vectorizador\n",
    "joblib.dump(vector, 'vectorizador_tfidf.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    1000\n",
      "1    1000\n",
      "2    1000\n",
      "3    1000\n",
      "4    1000\n",
      "5    1000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_balanced['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto: No solo siento decepcionado por mi falta de fuerza de voluntad y, como si tomé varios pasos, en términos de mi entrenamiento de culturismo, sino que siento los antojos de toda su fuerza y ​​ahora tengo que usar X la fuerza de voluntad para negarles\n",
      "Verdadero: 0, Predicho: 1\n",
      "Texto: Tragué un larabar, así que no me sentiría privado si fuera el tiempo de bocadillo y todo lo que tenían era algo como lo que ves arriba\n",
      "Verdadero: 4, Predicho: 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    if y_test.iloc[i] != y_predict[i]:\n",
    "        print(f\"Texto: {X.iloc[i]}\")\n",
    "        print(f\"Verdadero: {y_test.iloc[i]}, Predicho: {y_predict[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6883333333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.63      0.62       193\n",
      "           1       0.48      0.64      0.55       203\n",
      "           2       0.72      0.69      0.70       198\n",
      "           3       0.78      0.70      0.74       206\n",
      "           4       0.80      0.67      0.73       202\n",
      "           5       0.85      0.80      0.83       198\n",
      "\n",
      "    accuracy                           0.69      1200\n",
      "   macro avg       0.71      0.69      0.69      1200\n",
      "weighted avg       0.71      0.69      0.69      1200\n",
      "\n",
      "[[122  33  11  14  11   2]\n",
      " [ 25 129  27  11   7   4]\n",
      " [ 14  39 136   3   4   2]\n",
      " [ 20  26   7 144   8   1]\n",
      " [ 13  20   6   9 136  18]\n",
      " [  8  21   2   3   5 159]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_rf = RandomForestClassifier(n_estimators=300, criterion='gini',random_state=45)\n",
    "model_rf.fit(X_train,y_train)\n",
    "print(model_rf.score(X_test,y_test))\n",
    "y_predict = model_rf.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test,y_predict)\n",
    "cr = classification_report(y_test,y_predict)\n",
    "print(cr)\n",
    "print(cm)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7066666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.60      0.67       193\n",
      "           1       0.70      0.61      0.65       203\n",
      "           2       0.64      0.77      0.70       198\n",
      "           3       0.75      0.74      0.75       206\n",
      "           4       0.71      0.72      0.71       202\n",
      "           5       0.70      0.80      0.74       198\n",
      "\n",
      "    accuracy                           0.71      1200\n",
      "   macro avg       0.71      0.71      0.70      1200\n",
      "weighted avg       0.71      0.71      0.71      1200\n",
      "\n",
      "[[115  12  18  22  17   9]\n",
      " [  5 124  34  12  13  15]\n",
      " [  8  19 153   5   3  10]\n",
      " [ 11   9  12 153  13   8]\n",
      " [  5   6  11   8 145  27]\n",
      " [  8   6  10   3  13 158]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train,y_train)\n",
    "print(model.score(X_test,y_test))\n",
    "y_predict = model.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test,y_predict)\n",
    "cr = classification_report(y_test,y_predict)\n",
    "print(cr)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros: {'C': 10, 'max_iter': 8000, 'solver': 'liblinear'}\n",
      "0.745\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],  # Parámetro de regularización\n",
    "    'solver': ['liblinear', 'saga'],  # Solvers que podrías probar\n",
    "    'max_iter': [8000, 10000],  # Máximo de iteraciones\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(LogisticRegression(class_weight='balanced'), param_grid, cv=3, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mejor modelo encontrado\n",
    "print(\"Mejores parámetros:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluación del mejor modelo\n",
    "best_model = grid_search.best_estimator_\n",
    "print(best_model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['others', 'others', 'joy', 'others', 'others', 'joy', 'joy', 'joy', 'others', 'joy', 'anger', 'others', 'sadness', 'anger', 'anger', 'joy', 'others', 'others', 'anger', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'joy', 'others', 'joy', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'anger', 'others', 'others', 'joy', 'others', 'others', 'joy', 'others', 'others', 'joy', 'joy', 'joy', 'others', 'joy', 'anger', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'joy', 'others', 'others', 'others', 'others', 'others', 'joy', 'others', 'joy', 'others', 'others', 'others', 'others', 'disgust', 'sadness', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'sadness', 'fear', 'others', 'others', 'others', 'joy', 'joy', 'others', 'joy', 'others', 'joy', 'others', 'others', 'joy', 'joy', 'others', 'others', 'joy', 'joy', 'sadness', 'others', 'others', 'others', 'others', 'others', 'anger', 'joy', 'others', 'others', 'others', 'others', 'joy', 'others', 'others', 'others', 'anger', 'others', 'joy', 'joy', 'others', 'others', 'others', 'others', 'joy', 'others', 'joy', 'others', 'joy', 'others', 'joy', 'others', 'others', 'others', 'others', 'anger', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'anger', 'others', 'others', 'anger', 'joy', 'others', 'others', 'others', 'others', 'joy', 'joy', 'joy', 'joy', 'others', 'others', 'others', 'others', 'others', 'others', 'joy', 'joy', 'others', 'others', 'joy', 'others', 'others', 'joy', 'joy', 'sadness', 'others', 'others', 'joy', 'others', 'others', 'others', 'others', 'anger', 'others', 'joy', 'joy', 'joy', 'others', 'joy', 'others', 'others', 'others', 'anger', 'others', 'others', 'joy', 'joy', 'others', 'others', 'others', 'others', 'others', 'anger', 'anger', 'others', 'others', 'joy', 'joy', 'others', 'others', 'others', 'others', 'others', 'anger', 'joy', 'others', 'others', 'others', 'others', 'others', 'anger', 'others', 'others', 'joy', 'others', 'joy', 'others', 'joy', 'others', 'anger', 'others', 'others', 'others', 'others', 'joy', 'others', 'anger', 'others', 'others', 'joy', 'others', 'others', 'others', 'others', 'joy', 'joy', 'others', 'others', 'others', 'others', 'others', 'anger', 'others', 'others', 'others', 'joy', 'others', 'joy', 'joy', 'others', 'joy', 'others', 'others', 'anger', 'others', 'joy', 'others', 'others', 'joy', 'others', 'joy', 'others', 'others', 'others', 'joy', 'others', 'others', 'others', 'others', 'joy', 'joy', 'others', 'joy', 'others', 'joy', 'anger', 'others', 'others', 'others', 'anger', 'others', 'joy', 'others', 'others', 'others', 'others', 'others', 'joy', 'joy', 'others', 'others', 'others', 'joy', 'others', 'joy', 'others', 'joy', 'others', 'joy', 'others', 'others', 'others', 'others', 'joy', 'others', 'others', 'others', 'others', 'others', 'others', 'anger', 'others', 'others', 'others', 'others', 'joy', 'fear', 'others', 'joy', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'joy', 'others', 'others', 'others', 'joy', 'others', 'others', 'joy', 'others', 'joy', 'others', 'others', 'others', 'joy', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'joy', 'others', 'others', 'others', 'joy', 'joy', 'others', 'anger', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'joy', 'joy', 'others', 'others', 'others', 'others', 'others', 'joy', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'anger', 'joy', 'others', 'others', 'joy', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'joy', 'joy', 'others', 'others', 'others', 'others', 'others', 'joy', 'anger', 'others', 'others', 'others', 'joy', 'others', 'joy', 'others', 'others', 'others', 'others', 'others', 'others', 'joy', 'joy', 'others', 'others', 'joy', 'others', 'others', 'joy', 'anger', 'others', 'joy', 'others', 'others', 'others', 'others', 'anger', 'joy', 'joy', 'joy', 'joy', 'others', 'anger', 'others', 'anger', 'joy', 'others', 'others', 'others', 'anger', 'others', 'others', 'others', 'others', 'others', 'others', 'joy', 'joy', 'others', 'others', 'others', 'others', 'others', 'others', 'joy', 'others', 'joy', 'joy', 'others', 'joy', 'others', 'others', 'joy', 'others', 'others', 'others', 'others', 'others', 'anger', 'others', 'others', 'others', 'joy', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'joy', 'others', 'others', 'others', 'others', 'others', 'joy', 'joy', 'others', 'others', 'others', 'joy', 'others', 'others', 'others', 'anger', 'others', 'others', 'sadness', 'joy', 'others', 'others', 'others', 'joy', 'others', 'others', 'others', 'joy', 'others', 'others', 'joy', 'anger', 'others', 'others', 'others', 'joy', 'others', 'others', 'others', 'others', 'joy', 'anger', 'others', 'others', 'joy', 'others', 'others', 'others', 'joy', 'others', 'others', 'joy', 'others', 'anger', 'others', 'others', 'others', 'anger', 'others', 'others', 'joy', 'others', 'others', 'others', 'others', 'joy', 'joy', 'others', 'others', 'others', 'others', 'others', 'others', 'joy', 'joy', 'joy', 'others', 'others', 'others', 'sadness', 'joy', 'others', 'others', 'others', 'others', 'joy', 'others', 'others', 'anger', 'others', 'others', 'others', 'joy', 'joy', 'others', 'anger', 'joy', 'anger', 'others', 'others', 'others', 'joy', 'joy', 'others', 'others', 'others', 'others', 'others', 'joy', 'others', 'others', 'others', 'others', 'joy', 'others', 'others', 'others', 'others', 'joy', 'sadness', 'others', 'joy', 'others', 'others', 'others', 'joy', 'others', 'others', 'joy', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'anger', 'joy', 'others', 'others', 'anger', 'others', 'others', 'others', 'others', 'joy', 'others', 'joy', 'others', 'others', 'others', 'joy', 'others', 'sadness', 'others', 'anger', 'others', 'others', 'others', 'others', 'others', 'joy', 'others', 'joy', 'others', 'others', 'others', 'anger', 'joy', 'others', 'others', 'others', 'others', 'joy', 'others', 'others', 'others', 'others', 'anger', 'others', 'others', 'joy', 'joy', 'others', 'sadness', 'joy', 'others', 'joy', 'others', 'others', 'others', 'others', 'joy', 'others', 'others', 'others', 'sadness', 'others', 'anger', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'anger', 'others', 'joy', 'others', 'joy', 'others', 'others', 'joy', 'others', 'joy', 'others', 'others', 'others', 'sadness', 'sadness', 'others', 'others', 'others', 'others', 'others', 'anger', 'others', 'others', 'others', 'others', 'others', 'joy', 'joy', 'anger', 'others', 'others', 'joy', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'joy', 'others', 'others', 'others', 'others', 'anger', 'others', 'others', 'others', 'joy', 'others', 'others', 'joy', 'joy', 'joy', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'anger', 'others', 'anger', 'others', 'others', 'others', 'others', 'others', 'joy', 'joy', 'joy', 'others', 'others', 'others', 'joy', 'joy', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'joy', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'anger', 'others', 'joy', 'others', 'others', 'joy', 'others', 'others', 'others', 'sadness', 'others', 'joy', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'joy', 'others', 'others', 'others', 'joy', 'others', 'joy', 'others', 'others', 'joy', 'others', 'others', 'others', 'others', 'others', 'others', 'joy', 'joy', 'others', 'others', 'joy', 'joy', 'joy', 'others', 'anger', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'joy', 'surprise', 'joy', 'others', 'joy', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'anger', 'others', 'others', 'others', 'joy', 'others', 'others', 'others', 'others', 'others', 'others', 'joy', 'others', 'others', 'joy', 'joy', 'anger', 'anger', 'joy', 'joy', 'anger', 'others', 'others', 'anger', 'joy', 'others', 'others', 'anger', 'joy', 'anger', 'joy', 'others', 'others', 'joy', 'others', 'others', 'others', 'others', 'others', 'others', 'joy', 'joy', 'others', 'joy', 'others', 'others', 'others', 'others', 'joy', 'others', 'others', 'others', 'sadness', 'others', 'others', 'others', 'joy', 'anger', 'joy', 'others', 'others', 'others', 'others', 'joy', 'others', 'others', 'others', 'others', 'others', 'others', 'anger', 'anger', 'others', 'others', 'others', 'others', 'anger', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'anger', 'joy', 'others', 'others', 'others', 'others', 'joy', 'others', 'others', 'others', 'joy', 'others', 'others', 'anger', 'others', 'joy', 'others', 'anger', 'others', 'others', 'joy', 'joy']\n"
     ]
    }
   ],
   "source": [
    "target=[]\n",
    "for i in range(0,1000):\n",
    "    dicc_ordenado = sorted(dicc[i], key=lambda x: x['score'],reverse=True)\n",
    "    target.append(dicc_ordenado[0]['label'])\n",
    "\n",
    "print(target)   \n",
    "    \n",
    "    #for item in dicc_ordenado:\n",
    "        #print(item)\n",
    "        #target.append(item[0]['label'])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
